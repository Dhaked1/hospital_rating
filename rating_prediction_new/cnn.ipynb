{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84290019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64661070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Load and clean data\\ndata = pd.read_csv(\\'care.csv\\')\\n\\ninput_columns = [\\n    \\'H_COMP_7_A\\', \\'H_COMP_7_D_SD\\', \\'H_COMP_7_SA\\',\\n    \\'H_CT_MED_A\\', \\'H_CT_MED_D_SD\\', \\'H_CT_MED_SA\\',\\n    \\'H_CT_PREFER_A\\', \\'H_CT_PREFER_D_SD\\', \\'H_CT_PREFER_SA\\',\\n    \\'H_CT_UNDER_A\\', \\'H_CT_UNDER_D_SD\\', \\'H_CT_UNDER_SA\\'\\n]\\noutput_column = \\'H_COMP_7_STAR_RATING\\'\\n\\n# Preprocessing\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata = data.dropna()\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata = data.dropna()\\n\\nX = data[input_columns].values\\ny = to_categorical(data[output_column].values.astype(int) - 1)  # One-hot encoded, 0-4\\n\\n# Train-test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale features\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'care_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape input for CNN (samples, time steps, features per step)\\n# Since we have 12 features, treat each as a timestep with 1 feature\\nX_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\\nX_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\\n\\n# CNN Model\\nmodel = Sequential([\\n    Conv1D(64, kernel_size=3, activation=\\'relu\\', input_shape=(X_train_cnn.shape[1], 1)),\\n    MaxPooling1D(pool_size=2),\\n    Conv1D(32, kernel_size=3, activation=\\'relu\\'),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dropout(0.3),\\n    Dense(5, activation=\\'softmax\\')  # 5 classes\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train\\nmodel.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\\n\\n# Evaluate\\nloss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\\nprint(f\"Test Accuracy: {accuracy:.4f}\")\\n\\n# Save model\\nmodel.save(\\'care_cnn.keras\\')\\n\\n# Save model in pickle (optional but not typical for Keras)\\nwith open(\\'care_cnn.pkl\\', \\'wb\\') as f:\\n    pickle.dump(model, f)\\n\\nprint(\"CNN model and scaler saved successfully.\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''# Load and clean data\n",
    "data = pd.read_csv('care.csv')\n",
    "\n",
    "input_columns = [\n",
    "    'H_COMP_7_A', 'H_COMP_7_D_SD', 'H_COMP_7_SA',\n",
    "    'H_CT_MED_A', 'H_CT_MED_D_SD', 'H_CT_MED_SA',\n",
    "    'H_CT_PREFER_A', 'H_CT_PREFER_D_SD', 'H_CT_PREFER_SA',\n",
    "    'H_CT_UNDER_A', 'H_CT_UNDER_D_SD', 'H_CT_UNDER_SA'\n",
    "]\n",
    "output_column = 'H_COMP_7_STAR_RATING'\n",
    "\n",
    "# Preprocessing\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "X = data[input_columns].values\n",
    "y = to_categorical(data[output_column].values.astype(int) - 1)  # One-hot encoded, 0-4\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('care_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape input for CNN (samples, time steps, features per step)\n",
    "# Since we have 12 features, treat each as a timestep with 1 feature\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(32, kernel_size=3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(5, activation='softmax')  # 5 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('care_cnn.keras')\n",
    "\n",
    "# Save model in pickle (optional but not typical for Keras)\n",
    "with open('care_cnn.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"CNN model and scaler saved successfully.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77ea46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data = pd.read_csv(\\'clean.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\'H_CLEAN_HSP_A_P\\', \\'H_CLEAN_HSP_SN_P\\', \\'H_CLEAN_HSP_U_P\\']\\noutput_column = \\'H_CLEAN_STAR_RATING\\'\\n\\n# Filter and clean\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata = data.dropna()\\n\\n# Ensure numeric types\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata = data.dropna()\\n\\n# Extract features and target\\nX = data[input_columns].values\\ny = data[output_column].values\\ny = to_categorical(y - 1)  # One-hot encode from 0 to 4\\n\\n# Train/test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale features\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Save the scaler\\nwith open(\\'clean_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape for Conv1D: (samples, timesteps, features)\\nX_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\\nX_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\\n\\n# Build the CNN model\\nmodel = Sequential([\\n    Conv1D(32, kernel_size=2, activation=\\'relu\\', input_shape=(X_train_cnn.shape[1], 1)),\\n    MaxPooling1D(pool_size=1),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dense(32, activation=\\'relu\\'),\\n    Dense(5, activation=\\'softmax\\')  # Output: 5 star classes\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model\\nhistory = model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\\n\\n# Evaluate the model\\nloss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\\nprint(f\"Test Accuracy: {accuracy:.4f}\")\\n\\n# Save the CNN model\\nmodel.save(\\'clean_cnn.keras\\')\\n\\n# Save the scaler\\nwith open(\\'clean_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n\\nprint(\"CNN model and scaler saved successfully.\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "'''data = pd.read_csv('clean.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = ['H_CLEAN_HSP_A_P', 'H_CLEAN_HSP_SN_P', 'H_CLEAN_HSP_U_P']\n",
    "output_column = 'H_CLEAN_STAR_RATING'\n",
    "\n",
    "# Filter and clean\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "# Ensure numeric types\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Extract features and target\n",
    "X = data[input_columns].values\n",
    "y = data[output_column].values\n",
    "y = to_categorical(y - 1)  # One-hot encode from 0 to 4\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler\n",
    "with open('clean_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps, features)\n",
    "X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # Output: 5 star classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the CNN model\n",
    "model.save('clean_cnn.keras')\n",
    "\n",
    "# Save the scaler\n",
    "with open('clean_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "print(\"CNN model and scaler saved successfully.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9637e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the dataset\\ndata = pd.read_csv(\\'doctor_new.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\n    \\'H_COMP_2_A_P\\', \\'H_COMP_2_SN_P\\', \\'H_COMP_2_U_P\\',\\n    \\'H_DOCTOR_EXPLAIN_A_P\\', \\'H_DOCTOR_EXPLAIN_SN_P\\', \\'H_DOCTOR_EXPLAIN_U_P\\',\\n    \\'H_DOCTOR_LISTEN_A_P\\', \\'H_DOCTOR_LISTEN_SN_P\\', \\'H_DOCTOR_LISTEN_U_P\\',\\n    \\'H_DOCTOR_RESPECT_A_P\\', \\'H_DOCTOR_RESPECT_SN_P\\', \\'H_DOCTOR_RESPECT_U_P\\'\\n]\\noutput_column = \\'H_COMP_2_STAR_RATING\\'\\n\\n# Select relevant columns\\ndata = data[input_columns + [output_column]]\\n\\n# Handle missing values\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata = data.dropna()\\n\\n# Convert to numeric\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata = data.dropna()\\n\\n# Extract X and y\\nX = data[input_columns].values\\ny = data[output_column].values\\ny = to_categorical(y - 1)  # convert 1-5 stars to 0-4, then one-hot encode\\n\\n# Train/test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale features\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'doctor_new_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape for CNN input (samples, timesteps, features)\\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\\n\\n# Build CNN model\\nmodel = Sequential([\\n    Conv1D(64, kernel_size=3, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    MaxPooling1D(pool_size=2),\\n    Dropout(0.3),\\n    Conv1D(32, kernel_size=2, activation=\\'relu\\'),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dense(5, activation=\\'softmax\\')  # 5 star classes\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train\\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\\n\\n# Evaluate\\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\\nprint(f\"Test Accuracy: {accuracy:.4f}\")\\n\\n# Save the CNN model\\nmodel.save(\\'doctor_new_cnn.keras\\')\\n\\nprint(\"CNN model and scaler saved successfully.\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the dataset\n",
    "data = pd.read_csv('doctor_new.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = [\n",
    "    'H_COMP_2_A_P', 'H_COMP_2_SN_P', 'H_COMP_2_U_P',\n",
    "    'H_DOCTOR_EXPLAIN_A_P', 'H_DOCTOR_EXPLAIN_SN_P', 'H_DOCTOR_EXPLAIN_U_P',\n",
    "    'H_DOCTOR_LISTEN_A_P', 'H_DOCTOR_LISTEN_SN_P', 'H_DOCTOR_LISTEN_U_P',\n",
    "    'H_DOCTOR_RESPECT_A_P', 'H_DOCTOR_RESPECT_SN_P', 'H_DOCTOR_RESPECT_U_P'\n",
    "]\n",
    "output_column = 'H_COMP_2_STAR_RATING'\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[input_columns + [output_column]]\n",
    "\n",
    "# Handle missing values\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert to numeric\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Extract X and y\n",
    "X = data[input_columns].values\n",
    "y = data[output_column].values\n",
    "y = to_categorical(y - 1)  # convert 1-5 stars to 0-4, then one-hot encode\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('doctor_new_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for CNN input (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(32, kernel_size=2, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # 5 star classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the CNN model\n",
    "model.save('doctor_new_cnn.keras')\n",
    "\n",
    "print(\"CNN model and scaler saved successfully.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba10248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 1.4200 - val_accuracy: 0.7805 - val_loss: 0.7514\n",
      "Epoch 2/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7702 - loss: 0.6797 - val_accuracy: 0.8537 - val_loss: 0.3786\n",
      "Epoch 3/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8423 - loss: 0.4038 - val_accuracy: 0.8963 - val_loss: 0.2545\n",
      "Epoch 4/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8817 - loss: 0.2944 - val_accuracy: 0.9126 - val_loss: 0.2395\n",
      "Epoch 5/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9076 - loss: 0.2435 - val_accuracy: 0.9207 - val_loss: 0.1899\n",
      "Epoch 6/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8856 - loss: 0.2714 - val_accuracy: 0.9289 - val_loss: 0.1734\n",
      "Epoch 7/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9108 - loss: 0.2344 - val_accuracy: 0.9329 - val_loss: 0.1690\n",
      "Epoch 8/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9051 - loss: 0.2188 - val_accuracy: 0.9451 - val_loss: 0.1566\n",
      "Epoch 9/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9094 - loss: 0.2146 - val_accuracy: 0.9329 - val_loss: 0.1580\n",
      "Epoch 10/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9063 - loss: 0.2049 - val_accuracy: 0.9390 - val_loss: 0.1676\n",
      "Epoch 11/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8880 - loss: 0.2604 - val_accuracy: 0.9329 - val_loss: 0.1603\n",
      "Epoch 12/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9124 - loss: 0.2002 - val_accuracy: 0.9370 - val_loss: 0.1506\n",
      "Epoch 13/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9246 - loss: 0.1886 - val_accuracy: 0.9431 - val_loss: 0.1559\n",
      "Epoch 14/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9242 - loss: 0.1884 - val_accuracy: 0.9451 - val_loss: 0.1406\n",
      "Epoch 15/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8924 - loss: 0.2264 - val_accuracy: 0.9126 - val_loss: 0.1888\n",
      "Epoch 16/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9249 - loss: 0.1929 - val_accuracy: 0.9451 - val_loss: 0.1528\n",
      "Epoch 17/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9151 - loss: 0.1938 - val_accuracy: 0.9492 - val_loss: 0.1359\n",
      "Epoch 18/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9215 - loss: 0.1851 - val_accuracy: 0.9390 - val_loss: 0.1417\n",
      "Epoch 19/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9255 - loss: 0.2005 - val_accuracy: 0.9472 - val_loss: 0.1351\n",
      "Epoch 20/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9132 - loss: 0.1947 - val_accuracy: 0.9451 - val_loss: 0.1360\n",
      "Epoch 21/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9169 - loss: 0.1820 - val_accuracy: 0.9472 - val_loss: 0.1339\n",
      "Epoch 22/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9116 - loss: 0.2151 - val_accuracy: 0.9431 - val_loss: 0.1380\n",
      "Epoch 23/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9197 - loss: 0.1915 - val_accuracy: 0.9390 - val_loss: 0.1474\n",
      "Epoch 24/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9251 - loss: 0.1648 - val_accuracy: 0.9492 - val_loss: 0.1326\n",
      "Epoch 25/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9302 - loss: 0.1620 - val_accuracy: 0.9512 - val_loss: 0.1321\n",
      "Epoch 26/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9272 - loss: 0.1714 - val_accuracy: 0.9533 - val_loss: 0.1314\n",
      "Epoch 27/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9253 - loss: 0.1830 - val_accuracy: 0.9492 - val_loss: 0.1354\n",
      "Epoch 28/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9349 - loss: 0.1612 - val_accuracy: 0.9431 - val_loss: 0.1287\n",
      "Epoch 29/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9318 - loss: 0.1556 - val_accuracy: 0.9512 - val_loss: 0.1275\n",
      "Epoch 30/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9330 - loss: 0.1509 - val_accuracy: 0.9573 - val_loss: 0.1205\n",
      "Epoch 31/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9251 - loss: 0.1673 - val_accuracy: 0.9411 - val_loss: 0.1421\n",
      "Epoch 32/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9163 - loss: 0.1740 - val_accuracy: 0.9533 - val_loss: 0.1235\n",
      "Epoch 33/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9286 - loss: 0.1636 - val_accuracy: 0.9472 - val_loss: 0.1307\n",
      "Epoch 34/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9322 - loss: 0.1583 - val_accuracy: 0.9573 - val_loss: 0.1238\n",
      "Epoch 35/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9395 - loss: 0.1349 - val_accuracy: 0.9228 - val_loss: 0.1559\n",
      "Epoch 36/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9324 - loss: 0.1569 - val_accuracy: 0.9553 - val_loss: 0.1192\n",
      "Epoch 37/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.1625 - val_accuracy: 0.9431 - val_loss: 0.1283\n",
      "Epoch 38/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9304 - loss: 0.1493 - val_accuracy: 0.9431 - val_loss: 0.1290\n",
      "Epoch 39/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9319 - loss: 0.1664 - val_accuracy: 0.9533 - val_loss: 0.1217\n",
      "Epoch 40/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9237 - loss: 0.1603 - val_accuracy: 0.9431 - val_loss: 0.1323\n",
      "Epoch 41/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9305 - loss: 0.1503 - val_accuracy: 0.9492 - val_loss: 0.1204\n",
      "Epoch 42/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9287 - loss: 0.1626 - val_accuracy: 0.9309 - val_loss: 0.1494\n",
      "Epoch 43/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9478 - loss: 0.1415 - val_accuracy: 0.9533 - val_loss: 0.1178\n",
      "Epoch 44/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9339 - loss: 0.1578 - val_accuracy: 0.9573 - val_loss: 0.1199\n",
      "Epoch 45/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9383 - loss: 0.1364 - val_accuracy: 0.9533 - val_loss: 0.1168\n",
      "Epoch 46/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9444 - loss: 0.1416 - val_accuracy: 0.9492 - val_loss: 0.1331\n",
      "Epoch 47/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9365 - loss: 0.1466 - val_accuracy: 0.9512 - val_loss: 0.1226\n",
      "Epoch 48/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9363 - loss: 0.1487 - val_accuracy: 0.9329 - val_loss: 0.1785\n",
      "Epoch 49/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9310 - loss: 0.1524 - val_accuracy: 0.9573 - val_loss: 0.1189\n",
      "Epoch 50/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9421 - loss: 0.1456 - val_accuracy: 0.9512 - val_loss: 0.1176\n",
      "Test Accuracy: 0.9512\n",
      "CNN model and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Load the dataset\n",
    "data = pd.read_csv('help_new.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = [\n",
    "    'H_BATH_HELP_A_P', 'H_BATH_HELP_SN_P', 'H_BATH_HELP_U_P',\n",
    "    'H_CALL_BUTTON_A_P', 'H_CALL_BUTTON_SN_P', 'H_CALL_BUTTON_U_P',\n",
    "    'H_COMP_3_A_P', 'H_COMP_3_SN_P', 'H_COMP_3_U_P'\n",
    "]\n",
    "output_column = 'H_COMP_3_STAR_RATING'\n",
    "\n",
    "# Select and clean data\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Extract X and y\n",
    "X = data[input_columns].values\n",
    "y = data[output_column].values\n",
    "y = to_categorical(y - 1)  # 1-5 stars → 0-4 → one-hot\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('help_new_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(32, kernel_size=2, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # 5 star classes\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('help_new_cnn.keras')\n",
    "\n",
    "print(\"CNN model and scaler saved successfully.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5138a92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the dataset\\ndata = pd.read_csv(\\'left_hospital.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\'H_DISCH_HELP_N_P\\', \\'H_DISCH_HELP_Y_P\\', \\'H_SYMPTOMS_N_P\\', \\'H_SYMPTOMS_Y_P\\']\\noutput_column = \\'H_COMP_6_STAR_RATING\\'\\n\\n# Preprocess data\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata = data.dropna()\\n\\n# Convert to numeric\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata = data.dropna()\\n\\n# Prepare features and labels\\nX = data[input_columns].values\\ny = to_categorical(data[output_column].values - 1)  # Convert to 0-based and one-hot\\n\\n# Train/test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale features\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'left_hospital_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape for CNN input: (samples, timesteps, features)\\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\\n\\n# Build CNN model\\nmodel = Sequential([\\n    Conv1D(64, kernel_size=2, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    MaxPooling1D(pool_size=1),\\n    Dropout(0.3),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dense(5, activation=\\'softmax\\')  # 5 star classes\\n])\\n\\n# Compile model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train model\\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\\n\\n# Evaluate model\\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\\nprint(f\"Test Accuracy: {accuracy:.4f}\")\\n\\n# Save model\\nmodel.save(\\'left_hospital_cnn.keras\\')\\n\\nprint(\"CNN model and scaler saved successfully.\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the dataset\n",
    "data = pd.read_csv('left_hospital.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = ['H_DISCH_HELP_N_P', 'H_DISCH_HELP_Y_P', 'H_SYMPTOMS_N_P', 'H_SYMPTOMS_Y_P']\n",
    "output_column = 'H_COMP_6_STAR_RATING'\n",
    "\n",
    "# Preprocess data\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert to numeric\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Prepare features and labels\n",
    "X = data[input_columns].values\n",
    "y = to_categorical(data[output_column].values - 1)  # Convert to 0-based and one-hot\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('left_hospital_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for CNN input: (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # 5 star classes\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('left_hospital_cnn.keras')\n",
    "\n",
    "print(\"CNN model and scaler saved successfully.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4dbbda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Load the dataset\\ndata = pd.read_csv(\\'nurse_new.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\n    \\'H_COMP_1_A_P\\', \\'H_COMP_1_SN_P\\', \\'H_COMP_1_U_P\\',\\n    \\'H_NURSE_EXPLAIN_A_P\\', \\'H_NURSE_EXPLAIN_SN_P\\', \\'H_NURSE_EXPLAIN_U_P\\',\\n    \\'H_NURSE_LISTEN_A_P\\', \\'H_NURSE_LISTEN_SN_P\\', \\'H_NURSE_LISTEN_U_P\\',\\n    \\'H_NURSE_RESPECT_A_P\\', \\'H_NURSE_RESPECT_SN_P\\', \\'H_NURSE_RESPECT_U_P\\'\\n]\\noutput_column = \\'H_COMP_1_STAR_RATING\\'\\n\\n# Select and clean the data\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata = data.dropna()\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata = data.dropna()\\n\\n# Prepare input and output\\nX = data[input_columns].values\\ny = data[output_column].values\\ny = to_categorical(y - 1)  # One-hot encode (1–5 → 0–4)\\n\\n# Train/test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale input\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'nurse_new_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape for CNN input (samples, timesteps, features)\\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\\n\\n# Build CNN model\\nmodel = Sequential([\\n    Conv1D(64, kernel_size=3, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    MaxPooling1D(pool_size=2),\\n    Dropout(0.3),\\n    Conv1D(32, kernel_size=2, activation=\\'relu\\'),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dense(5, activation=\\'softmax\\')  # 5 star classes\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train model\\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\\n\\n# Evaluate\\nloss, accuracy = model.evaluate(X_test, y_test, verbose=0)\\nprint(f\"Test Accuracy: {accuracy:.4f}\")\\n\\n# Save model\\nmodel.save(\\'nurse_new_cnn.keras\\')\\n\\nprint(\"✅ CNN model and scaler saved successfully.\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Load the dataset\n",
    "data = pd.read_csv('nurse_new.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = [\n",
    "    'H_COMP_1_A_P', 'H_COMP_1_SN_P', 'H_COMP_1_U_P',\n",
    "    'H_NURSE_EXPLAIN_A_P', 'H_NURSE_EXPLAIN_SN_P', 'H_NURSE_EXPLAIN_U_P',\n",
    "    'H_NURSE_LISTEN_A_P', 'H_NURSE_LISTEN_SN_P', 'H_NURSE_LISTEN_U_P',\n",
    "    'H_NURSE_RESPECT_A_P', 'H_NURSE_RESPECT_SN_P', 'H_NURSE_RESPECT_U_P'\n",
    "]\n",
    "output_column = 'H_COMP_1_STAR_RATING'\n",
    "\n",
    "# Select and clean the data\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data = data.dropna()\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Prepare input and output\n",
    "X = data[input_columns].values\n",
    "y = data[output_column].values\n",
    "y = to_categorical(y - 1)  # One-hot encode (1–5 → 0–4)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale input\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('nurse_new_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for CNN input (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    Conv1D(32, kernel_size=2, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # 5 star classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('nurse_new_cnn.keras')\n",
    "\n",
    "print(\"✅ CNN model and scaler saved successfully.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521b1d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the dataset\\ndata = pd.read_csv(\\'quite.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\'H_QUIET_HSP_A_P\\', \\'H_QUIET_HSP_SN_P\\', \\'H_QUIET_HSP_U_P\\']\\noutput_column = \\'H_QUIET_STAR_RATING\\'\\n\\n# Select relevant columns and clean\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata.dropna(inplace=True)\\n\\n# Convert to numeric\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata.dropna(inplace=True)\\n\\n# Extract features and labels\\nX = data[input_columns].values\\ny = to_categorical(data[output_column].values.astype(int) - 1)  # One-hot encoding (0–4)\\n\\n# Train-test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale inputs\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'quiet_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape input for Conv1D: (samples, timesteps, features)\\nX_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\\nX_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\\n\\n# Build CNN model\\nmodel = Sequential([\\n    Conv1D(filters=64, kernel_size=2, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    MaxPooling1D(pool_size=1),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dropout(0.3),\\n    Dense(5, activation=\\'softmax\\')  # 5 classes for 1-5 star ratings\\n])\\n\\n# Compile\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train\\nmodel.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\\n\\n# Evaluate\\nloss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)\\nprint(f\"Test Accuracy: {acc:.4f}\")\\n\\n# Save model\\nmodel.save(\\'quiet_cnn.keras\\')\\n\\nprint(\"✅ CNN model and scaler saved as \\'quiet_cnn.keras\\' and \\'quiet_scaler.pkl\\'\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the dataset\n",
    "data = pd.read_csv('quite.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = ['H_QUIET_HSP_A_P', 'H_QUIET_HSP_SN_P', 'H_QUIET_HSP_U_P']\n",
    "output_column = 'H_QUIET_STAR_RATING'\n",
    "\n",
    "# Select relevant columns and clean\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert to numeric\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[input_columns].values\n",
    "y = to_categorical(data[output_column].values.astype(int) - 1)  # One-hot encoding (0–4)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('quiet_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape input for Conv1D: (samples, timesteps, features)\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(5, activation='softmax')  # 5 classes for 1-5 star ratings\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('quiet_cnn.keras')\n",
    "\n",
    "print(\"✅ CNN model and scaler saved as 'quiet_cnn.keras' and 'quiet_scaler.pkl'\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84bdd5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load the dataset\\ndata = pd.read_csv(\\'recommend.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\'H_RECMND_DN\\', \\'H_RECMND_DY\\', \\'H_RECMND_PY\\']\\noutput_column = \\'H_COMP_1_STAR_RATING\\'\\n\\n# Select relevant columns and clean\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata.dropna(inplace=True)\\n\\n# Convert to numeric\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata.dropna(inplace=True)\\n\\n# Extract features and labels\\nX = data[input_columns].values\\ny = to_categorical(data[output_column].astype(int) - 1, num_classes=5)  # 0–4 classes\\n\\n# Train-test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Scale inputs\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'recommend_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape for Conv1D: (samples, timesteps, features)\\nX_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\\nX_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\\n\\n# Build CNN model\\nmodel = Sequential([\\n    Conv1D(filters=64, kernel_size=2, activation=\\'relu\\', input_shape=(X_train.shape[1], 1)),\\n    MaxPooling1D(pool_size=1),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dropout(0.3),\\n    Dense(5, activation=\\'softmax\\')  # 5 output classes (1–5 star rating)\\n])\\n\\n# Compile model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train model\\nmodel.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\\n\\n# Evaluate model\\nloss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)\\nprint(f\"Test Accuracy: {acc:.4f}\")\\n\\n# Save the trained CNN model\\nmodel.save(\\'recommend_cnn.keras\\')\\n\\nprint(\"✅ CNN model and scaler saved as \\'recommend_cnn.keras\\' and \\'recommend_scaler.pkl\\'\")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load the dataset\n",
    "data = pd.read_csv('recommend.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = ['H_RECMND_DN', 'H_RECMND_DY', 'H_RECMND_PY']\n",
    "output_column = 'H_COMP_1_STAR_RATING'\n",
    "\n",
    "# Select relevant columns and clean\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert to numeric\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data[input_columns].values\n",
    "y = to_categorical(data[output_column].astype(int) - 1, num_classes=5)  # 0–4 classes\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('recommend_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps, features)\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(5, activation='softmax')  # 5 output classes (1–5 star rating)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "loss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Save the trained CNN model\n",
    "model.save('recommend_cnn.keras')\n",
    "\n",
    "print(\"✅ CNN model and scaler saved as 'recommend_cnn.keras' and 'recommend_scaler.pkl'\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461e4562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load dataset\\ndata = pd.read_csv(\\'staff_new.csv\\')\\n\\n# Define input and output columns\\ninput_columns = [\\n    \\'H_COMP_5_A_P\\', \\'H_COMP_5_SN_P\\', \\'H_COMP_5_U_P\\',\\n    \\'H_MED_FOR_A_P\\', \\'H_MED_FOR_SN_P\\', \\'H_MED_FOR_U_P\\',\\n    \\'H_SIDE_EFFECTS_A_P\\', \\'H_SIDE_EFFECTS_SN_P\\', \\'H_SIDE_EFFECTS_U_P\\'\\n]\\noutput_column = \\'H_COMP_5_STAR_RATING\\'\\n\\n# Clean data\\ndata = data[input_columns + [output_column]]\\ndata.replace([\\'Not Available\\', \\'\\', \\'NA\\', \\'N/A\\'], np.nan, inplace=True)\\ndata.dropna(inplace=True)\\n\\n# Convert all columns to numeric\\nfor col in input_columns:\\n    data[col] = pd.to_numeric(data[col], errors=\\'coerce\\')\\ndata[output_column] = pd.to_numeric(data[output_column], errors=\\'coerce\\')\\ndata.dropna(inplace=True)\\n\\n# Prepare features and labels\\nX = data[input_columns].values\\ny = to_categorical(data[output_column].astype(int) - 1)  # 0-based labels\\n\\n# Split data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Standardize features\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Save scaler\\nwith open(\\'staff_scaler.pkl\\', \\'wb\\') as f:\\n    pickle.dump(scaler, f)\\n\\n# Reshape input for Conv1D (samples, timesteps, features)\\nX_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\\nX_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\\n\\n# Build CNN model\\nmodel = Sequential([\\n    Conv1D(filters=64, kernel_size=2, activation=\\'relu\\', input_shape=(X_train_cnn.shape[1], 1)),\\n    MaxPooling1D(pool_size=1),\\n    Flatten(),\\n    Dense(64, activation=\\'relu\\'),\\n    Dropout(0.3),\\n    Dense(5, activation=\\'softmax\\')  # 5 classes (1-5 star ratings)\\n])\\n\\n# Compile\\nmodel.compile(optimizer=\\'adam\\', loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train\\nmodel.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\\n\\n# Evaluate\\nloss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)\\nprint(f\"Test Accuracy: {acc:.4f}\")\\n\\n# Save model\\nmodel.save(\\'staff_new_cnn.keras\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Load dataset\n",
    "data = pd.read_csv('staff_new.csv')\n",
    "\n",
    "# Define input and output columns\n",
    "input_columns = [\n",
    "    'H_COMP_5_A_P', 'H_COMP_5_SN_P', 'H_COMP_5_U_P',\n",
    "    'H_MED_FOR_A_P', 'H_MED_FOR_SN_P', 'H_MED_FOR_U_P',\n",
    "    'H_SIDE_EFFECTS_A_P', 'H_SIDE_EFFECTS_SN_P', 'H_SIDE_EFFECTS_U_P'\n",
    "]\n",
    "output_column = 'H_COMP_5_STAR_RATING'\n",
    "\n",
    "# Clean data\n",
    "data = data[input_columns + [output_column]]\n",
    "data.replace(['Not Available', '', 'NA', 'N/A'], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert all columns to numeric\n",
    "for col in input_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data[output_column] = pd.to_numeric(data[output_column], errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = data[input_columns].values\n",
    "y = to_categorical(data[output_column].astype(int) - 1)  # 0-based labels\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('staff_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape input for Conv1D (samples, timesteps, features)\n",
    "X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(5, activation='softmax')  # 5 classes (1-5 star ratings)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train_cnn, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('staff_new_cnn.keras')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e0a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.4365 - loss: 1.4742 - val_accuracy: 0.6240 - val_loss: 1.0004\n",
      "Epoch 2/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6979 - loss: 0.9089 - val_accuracy: 0.8069 - val_loss: 0.5973\n",
      "Epoch 3/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8350 - loss: 0.5438 - val_accuracy: 0.8720 - val_loss: 0.4039\n",
      "Epoch 4/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8563 - loss: 0.4014 - val_accuracy: 0.8902 - val_loss: 0.3212\n",
      "Epoch 5/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8691 - loss: 0.3455 - val_accuracy: 0.8740 - val_loss: 0.3019\n",
      "Epoch 6/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8573 - loss: 0.3180 - val_accuracy: 0.8862 - val_loss: 0.2806\n",
      "Epoch 7/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8718 - loss: 0.3077 - val_accuracy: 0.8821 - val_loss: 0.2755\n",
      "Epoch 8/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8711 - loss: 0.2916 - val_accuracy: 0.8882 - val_loss: 0.2713\n",
      "Epoch 9/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8662 - loss: 0.3248 - val_accuracy: 0.8943 - val_loss: 0.2665\n",
      "Epoch 10/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8889 - loss: 0.2803 - val_accuracy: 0.8638 - val_loss: 0.3150\n",
      "Epoch 11/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8897 - loss: 0.2684 - val_accuracy: 0.8720 - val_loss: 0.2817\n",
      "Epoch 12/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8849 - loss: 0.2737 - val_accuracy: 0.8679 - val_loss: 0.2802\n",
      "Epoch 13/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8970 - loss: 0.2679 - val_accuracy: 0.8984 - val_loss: 0.2568\n",
      "Epoch 14/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8814 - loss: 0.2766 - val_accuracy: 0.8963 - val_loss: 0.2552\n",
      "Epoch 15/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8929 - loss: 0.2640 - val_accuracy: 0.8862 - val_loss: 0.2651\n",
      "Epoch 16/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8860 - loss: 0.2749 - val_accuracy: 0.8984 - val_loss: 0.2462\n",
      "Epoch 17/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8790 - loss: 0.2823 - val_accuracy: 0.8963 - val_loss: 0.2532\n",
      "Epoch 18/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8829 - loss: 0.2636 - val_accuracy: 0.8923 - val_loss: 0.2580\n",
      "Epoch 19/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8890 - loss: 0.2720 - val_accuracy: 0.8740 - val_loss: 0.2820\n",
      "Epoch 20/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8747 - loss: 0.2717 - val_accuracy: 0.8963 - val_loss: 0.2621\n",
      "Epoch 21/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8953 - loss: 0.2508 - val_accuracy: 0.8841 - val_loss: 0.2608\n",
      "Epoch 22/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8883 - loss: 0.2714 - val_accuracy: 0.9045 - val_loss: 0.2580\n",
      "Epoch 23/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8930 - loss: 0.2690 - val_accuracy: 0.8923 - val_loss: 0.2630\n",
      "Epoch 24/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8710 - loss: 0.2901 - val_accuracy: 0.8740 - val_loss: 0.2691\n",
      "Epoch 25/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8803 - loss: 0.2749 - val_accuracy: 0.8841 - val_loss: 0.2696\n",
      "Epoch 26/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8872 - loss: 0.2715 - val_accuracy: 0.8902 - val_loss: 0.2504\n",
      "Epoch 27/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9043 - loss: 0.2494 - val_accuracy: 0.8923 - val_loss: 0.2497\n",
      "Epoch 28/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8850 - loss: 0.2678 - val_accuracy: 0.9085 - val_loss: 0.2514\n",
      "Epoch 29/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9040 - loss: 0.2406 - val_accuracy: 0.8943 - val_loss: 0.2466\n",
      "Epoch 30/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8942 - loss: 0.2489 - val_accuracy: 0.8821 - val_loss: 0.2613\n",
      "Epoch 31/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8817 - loss: 0.2998 - val_accuracy: 0.9106 - val_loss: 0.2432\n",
      "Epoch 32/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8929 - loss: 0.2500 - val_accuracy: 0.8720 - val_loss: 0.2780\n",
      "Epoch 33/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8948 - loss: 0.2609 - val_accuracy: 0.8882 - val_loss: 0.2585\n",
      "Epoch 34/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8943 - loss: 0.2445 - val_accuracy: 0.8963 - val_loss: 0.2555\n",
      "Epoch 35/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8862 - loss: 0.2647 - val_accuracy: 0.8943 - val_loss: 0.2499\n",
      "Epoch 36/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8942 - loss: 0.2629 - val_accuracy: 0.8963 - val_loss: 0.2600\n",
      "Epoch 37/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8888 - loss: 0.2480 - val_accuracy: 0.8801 - val_loss: 0.2620\n",
      "Epoch 38/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8906 - loss: 0.2493 - val_accuracy: 0.8882 - val_loss: 0.2876\n",
      "Epoch 39/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8899 - loss: 0.2717 - val_accuracy: 0.8984 - val_loss: 0.2453\n",
      "Epoch 40/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8905 - loss: 0.2629 - val_accuracy: 0.8984 - val_loss: 0.2455\n",
      "Epoch 41/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8975 - loss: 0.2564 - val_accuracy: 0.8902 - val_loss: 0.2453\n",
      "Epoch 42/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8894 - loss: 0.2663 - val_accuracy: 0.8780 - val_loss: 0.2730\n",
      "Epoch 43/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8820 - loss: 0.2815 - val_accuracy: 0.8923 - val_loss: 0.2552\n",
      "Epoch 44/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8986 - loss: 0.2467 - val_accuracy: 0.8780 - val_loss: 0.2637\n",
      "Epoch 45/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9022 - loss: 0.2361 - val_accuracy: 0.8862 - val_loss: 0.2719\n",
      "Epoch 46/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8873 - loss: 0.2554 - val_accuracy: 0.8760 - val_loss: 0.2662\n",
      "Epoch 47/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9049 - loss: 0.2311 - val_accuracy: 0.8943 - val_loss: 0.2481\n",
      "Epoch 48/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8871 - loss: 0.2614 - val_accuracy: 0.8862 - val_loss: 0.2473\n",
      "Epoch 49/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8809 - loss: 0.2764 - val_accuracy: 0.8943 - val_loss: 0.2500\n",
      "Epoch 50/50\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8783 - loss: 0.2700 - val_accuracy: 0.8882 - val_loss: 0.2458\n",
      "Test Accuracy: 0.8862\n",
      "✅ CNN model and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('rate.csv')\n",
    "\n",
    "# Replace 'Not Available' with NaN, then drop incomplete rows\n",
    "data.replace('Not Available', np.nan, inplace=True)\n",
    "data = data.dropna(subset=[\n",
    "    'H_HSP_RATING_0_6',\n",
    "    'H_HSP_RATING_7_8',\n",
    "    'H_HSP_RATING_9_10',\n",
    "    'H_HSP_RATING_STAR_RATING'\n",
    "])\n",
    "\n",
    "# Convert columns to numeric\n",
    "for col in ['H_HSP_RATING_0_6', 'H_HSP_RATING_7_8', 'H_HSP_RATING_9_10', 'H_HSP_RATING_STAR_RATING']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "# Prepare input features (X) and target (y)\n",
    "X = data[['H_HSP_RATING_0_6', 'H_HSP_RATING_7_8', 'H_HSP_RATING_9_10']].values\n",
    "y = data['H_HSP_RATING_STAR_RATING'].astype(int).values - 1  # zero-based (1→0, 5→4)\n",
    "y = to_categorical(y, num_classes=5)                        # one-hot encode, 5 classes\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for future use\n",
    "with open('star_rating_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps=3, channels=1)\n",
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(3, 1)),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(filters=32, kernel_size=2, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(5, activation='softmax')   # 5 star classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the CNN model\n",
    "model.save('hospital_star_rating_cnn.keras')\n",
    "\n",
    "print(\"✅ CNN model and scaler saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1a404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
